---
title: "Cross Validation"
author: "Arielle"
date: "11/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library (tidyverse)
library (viridis)
library (rvest)
library (p8105.datasets)
library (mgcv)
library (modelr)

set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

Generate a dataset 

```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - .3) ^ 2 + rnorm(100, 0, .3)
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + theme_bw()
```

Training and testing 

```{r}
train_df = sample_frac(nonlin_df, size = .8) 
test_df = anti_join(nonlin_df, train_df, by = "id")

ggplot(train_df, aes(x = x, y = y)) + 
  geom_point() + 
  geom_point(data = test_df, color = "red")
```

fit three models with varying goodness 


```{r}
linear_mod = lm(y~x, data = train_df)
smooth_mod = mgcv::gam(y~s(x), data = train_df)
wiggly_mod = mgcv::gam(y~s(x, k = 30), sp = 10e-6, data = train_df)
```

using linear
```{r}
train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + 
  geom_line(aes(y = pred), color = "red")
```
The linear model missed the curve completely

using the smooth
```{r}
train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + 
  geom_line(aes(y = pred), color = "red")
```
Looking realllll noice 

using wiggly

```{r}
train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + 
  geom_line(aes(y = pred), color = "red")
```
The wiggly model is a little wiggly not the best 


Doing some cross validation- smaller number the better 
```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)

rmse(linear_mod, train_df)
rmse(smooth_mod, train_df)
rmse(wiggly_mod, train_df)
```

For this data set, it looks good but this does not apply to other data sets that are not seen as of yet. Will always pick the model that does best in the training dataset. We want to say something about the future data set not necessarily this dataset. 

We finna formalize now 

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100)
```

one note anout the resample

```{r}
cv_df %>% 
  pull (train) %>% .[[1]] %>% 
  as_tibble 
```

The data sets are all different from each other- we dont want the memory- this makes it possible to fit models

```{r}
cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

Try fitting the linear model of all these 

```{r}
cv_results =
    cv_df %>% 
    mutate(
      linear_mods  = map(train, ~lm(y ~ x, data = .x)),
      smooth_mods  = map(train, ~mgcv::gam(y ~ s(x), data = .x)),
      wiggly_mods  = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))) %>% 
  mutate(rmse_linear = map2_dbl(linear_mods, test, ~rmse(model = .x, data = .y)),
         rmse_smooth = map2_dbl(smooth_mods, test, ~rmse(model = .x, data = .y)),
         rmse_wiggly = map2_dbl(wiggly_mods, test, ~rmse(model = .x, data = .y)))
```

visualize

```{r}
cv_results %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
We would pick the smooth model

```{r}
child_growth = read_csv("data /nepalese_children.csv")
```

```{r}
child_growth %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .5)
```


